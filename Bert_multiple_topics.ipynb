{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863c8f50-6c4b-41ba-91f7-23b719116880",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9ae77-21e4-4bb5-9818-f7c3a023d65b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pandas\n",
    "!pip install awswrangler\n",
    "!pip install pyspellchecker\n",
    "!pip install emoji\n",
    "!pip install swifter\n",
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "!conda uninstall -y TBB\n",
    "!pip install tbb>=2019.0\n",
    "!pip install bertopic\n",
    "!pip install gensim\n",
    "!pip install unidecode\n",
    "!pip install --upgrade sentence-transformers\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f8cd9-501f-4155-930e-ccac1e1bba1d",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba231f0-f627-43e3-9159-bf8d186ae932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spellchecker import SpellChecker\n",
    "import emoji\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import unidecode\n",
    "from dataclasses import dataclass\n",
    "import awswrangler as wr\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from bertopic import BERTopic\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import transformers\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c400c1-e0e9-4796-8f5f-2303a9b680e7",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7255a15-9e00-46b7-92e5-e8bae2224ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class emojiTransformer(BaseEstimator, TransformerMixin):\n",
    "    column: str\n",
    "    \n",
    "    def deEmojify(self, text) : \n",
    "        regrex_pattern = re.compile(pattern = \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags = re.UNICODE)\n",
    "        return regrex_pattern.sub(r'',text)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        emoji_package_removal = X[self.column].apply(lambda x: emoji.replace_emoji(x, replace=''))\n",
    "        # regex_removal = pd.DataFrame(X.apply(lambda x: self.deEmojify(text=x)))\n",
    "        return pd.DataFrame(emoji_package_removal, columns=[self.column])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "@dataclass\n",
    "class lowercaseTransformer(BaseEstimator, TransformerMixin):  \n",
    "    column: str\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X[self.column].apply(lambda x: x.lower()), columns=[self.column])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "@dataclass\n",
    "class accentsTransformer(BaseEstimator, TransformerMixin):  \n",
    "    column: str\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X[self.column].apply(lambda x: unidecode.unidecode(x)),\n",
    "                            columns=[self.column])\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "@dataclass\n",
    "class numberFilterTransformer(BaseEstimator, TransformerMixin):  \n",
    "    column: str\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X[self.column].apply(lambda x: re.sub(r'\\d+', '', x)),\n",
    "                            columns=[self.column])\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "@dataclass\n",
    "class punctuationFilterTransformer(BaseEstimator, TransformerMixin):  \n",
    "    column: str\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X[self.column].apply(lambda x: re.sub(r'[^\\w\\s]','', x)),\n",
    "                            columns=[self.column])\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "@dataclass\n",
    "class spellTransformer(BaseEstimator, TransformerMixin):  \n",
    "    column: str\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        spell = SpellChecker(language='pt') \n",
    "        return pd.DataFrame(X[self.column].swifter.apply(lambda x: spell.correction(x)))\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "@dataclass\n",
    "class stopwordsTransformer(BaseEstimator, TransformerMixin):  \n",
    "    column: str\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "        return pd.DataFrame(X[self.column].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords])),\n",
    "                            columns=[self.column])\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "@dataclass\n",
    "class wordTokenizerTransformer(BaseEstimator, TransformerMixin):  \n",
    "    column: str\n",
    "    ngram: int=1\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X[self.column].apply(lambda x: word_tokenize(x)),\n",
    "                            columns=[self.column])\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "@dataclass\n",
    "class removerTransformer(BaseEstimator, TransformerMixin):  \n",
    "    column: str\n",
    "    words = ['ok',\n",
    "             'okay',\n",
    "             'hum']\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "\n",
    "        return pd.DataFrame(X[self.column].apply(lambda x: ' '.join([word for word in x.split() if word not in self.words])),\n",
    "                            columns=[self.column])\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c5884-6798-4cdb-820f-702f68865cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformation_column = 'ds_message'\n",
    "bert_steps = [\n",
    " ('remove_emojis', emojiTransformer(transformation_column)),\n",
    " ('lower_case', lowercaseTransformer(transformation_column)),\n",
    " ('accents', accentsTransformer(transformation_column)),\n",
    " ('number_filter', numberFilterTransformer(transformation_column)),\n",
    " ('punctuation_filter', punctuationFilterTransformer(transformation_column)),\n",
    " # ('remove_words', removerTransformer(transformation_column)),\n",
    " ('stopwords', stopwordsTransformer(transformation_column)),\n",
    "]\n",
    "\n",
    "bert_pipe = Pipeline(steps=bert_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e30bf-3f76-4fe1-8cf4-28054fa50afc",
   "metadata": {},
   "source": [
    "# Global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde84a27-0110-4508-a228-31f739f5eeba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_date = '2023-01-01'\n",
    "max_date = '2024-02-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95235f-9495-4df1-8aa3-97f93b161d6d",
   "metadata": {},
   "source": [
    "## Query empréstimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb91896-2a44-4af0-81f9-607e1ec8c644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "\n",
    "  \n",
    "\"\"\"\n",
    "# Define staging_path\n",
    "staging_path = \n",
    "\n",
    "# Define boto3 session\n",
    "boto3_session = boto3.Session(region_name=\"sa-east-1\")\n",
    "\n",
    "# Clean temporary folder\n",
    "wr.s3.delete_objects(staging_path, boto3_session=boto3_session)\n",
    "\n",
    "# Run query\n",
    "raw_data = wr.athena.read_sql_query(\n",
    "    sql=query,\n",
    "    database=None,\n",
    "    ctas_approach=False,\n",
    "    s3_output=staging_path,\n",
    ")\n",
    "wr.s3.delete_objects(staging_path, boto3_session=boto3_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17829b-b271-497b-a4ba-4faeaab1c285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define staging_path\n",
    "staging_path = \"\"\n",
    "\n",
    "# Define boto3 session\n",
    "boto3_session = boto3.Session(region_name=\"sa-east-1\")\n",
    "\n",
    "# Clean temporary folder\n",
    "wr.s3.delete_objects(staging_path, boto3_session=boto3_session)\n",
    "\n",
    "query = \"\"\"\"\"\"\n",
    "\n",
    "# Run query\n",
    "tmp = wr.athena.read_sql_query(\n",
    "    sql=query,\n",
    "    database=None,\n",
    "    ctas_approach=False,\n",
    "    s3_output=staging_path,\n",
    ")\n",
    "wr.s3.delete_objects(staging_path, boto3_session=boto3_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba8bcd4-2c3f-4811-af5e-5160c5fab328",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f0cae-8361-479b-a981-7d7d8353d4f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand_dict_values(dictionary):\n",
    "    keys, values = zip(*dictionary.items())\n",
    "    expanded_values = list(product(*values))\n",
    "\n",
    "    expanded_dicts = []\n",
    "    for val in expanded_values:\n",
    "        expanded_dicts.append(dict(zip(keys, val)))\n",
    "\n",
    "    return expanded_dicts\n",
    "\n",
    "def merge_consecutive_chats(raw_data, tag):\n",
    "    df = raw_data[raw_data.ds_tags==tag].copy()\n",
    "    df = df.sort_values(['id_issue','id_customer','dt_envio_mensagem','dt_criacao_chat'])\n",
    "    display(df.shape)\n",
    "    df['shift'] = (df['ds_entidade'] != df['ds_entidade'].shift())\n",
    "    df['group'] = df['shift'].apply(lambda x: 0 if pd.isna(x) or x!=True else 1).cumsum()\n",
    "    display(df.shape)\n",
    "    df = df.groupby(['id_issue','id_customer', 'ds_entidade', 'group','ds_tags']).agg(\n",
    "        ds_message=('ds_message', ' '.join),\n",
    "        dt_criacao_chat=('dt_criacao_chat', 'min'),\n",
    "        dt_envio_mensagem=('dt_envio_mensagem', 'min')\n",
    "    )\n",
    "    display(df.shape)\n",
    "    df = df.sort_values(['id_issue','id_customer','group','dt_criacao_chat','dt_envio_mensagem']).reset_index()\n",
    "    display(df.shape)\n",
    "    df = df[df['ds_entidade']!='automação']\n",
    "    return df\n",
    "\n",
    "def write_model(topic_model,tag,params,entity,outlier=False):\n",
    "    p = params\n",
    "    if outlier:\n",
    "        with open(f\"{tag}_ngram_{p['n_gram_range'][0]}{p['n_gram_range'][1]}_nr_{p['nr_topics']}_min_{p['min_topic_size']}_{entity}_outlier.pkl\", 'wb') as file:    \n",
    "                pickle.dump(topic_model, file)\n",
    "    else:\n",
    "        with open(f\"{tag}_ngram_{p['n_gram_range'][0]}{p['n_gram_range'][1]}_nr_{p['nr_topics']}_min_{p['min_topic_size']}_{entity}.pkl\", 'wb') as file:    \n",
    "                pickle.dump(topic_model, file)\n",
    "                \n",
    "\n",
    "\n",
    "def load_models(tag,\n",
    "                entity,\n",
    "                dimension_params,\n",
    "                clustering_params,\n",
    "                vectorizer_params,\n",
    "                ctfidf_params,\n",
    "                best_param = True\n",
    "               ):\n",
    "    count = 0\n",
    "    for _dim_model, _dim_model_params in dimension_params.items():\n",
    "        for _d_p in expand_dict_values(_dim_model_params):\n",
    "            for _cluster_model, _cluster_model_params in clustering_params.items():\n",
    "                for _c_p in expand_dict_values(_cluster_model_params):\n",
    "                    for _vec_model, _vec_params in vectorizer_params.items():\n",
    "                        for _v_p in expand_dict_values(_vec_params):                    \n",
    "                            for _idf_model, _idf_params in ctfidf_params.items():\n",
    "                                for _i_p in expand_dict_values(_idf_params):\n",
    "                                    try:\n",
    "                                        print('Loading...')\n",
    "                                        file_path = f\"models/{unidecode.unidecode(tag.lower().replace(' ','_').replace('-','').replace('|',''))}__\"+\\\n",
    "                                              f\"{entity}__\"+\\\n",
    "                                              f\"m__d_{_dim_model}_{'_'.join([str(i) for i in list(_d_p.values())])}\"+\\\n",
    "                                              f\"__c_{_cluster_model}_{'_'.join([str(i) for i in list(_c_p.values())])}\"+\\\n",
    "                                              f\"__v_{_vec_model}_{'_'.join([str(i) for i in list(_v_p.values())])}\"+\\\n",
    "                                              f\"__i_{_idf_model}_{'_'.join([str(i) for i in list(_i_p.values())])}.pkl\"\n",
    "                                        print(file_path)\n",
    "                                        with open(file_path, 'rb') as file:\n",
    "                                            topic_model = pickle.load(file)\n",
    "                                            display('Total chats: ', topic_model.get_topic_info()['Count'].sum())\n",
    "                                            topic_info = topic_model.get_topic_info()\n",
    "                                            # mask = ~topic_info.Representation.apply(lambda x: any(item in x for item in post_filter))\n",
    "                                            # display(topic_info[mask].head(7))\n",
    "                                            display(topic_info.head(10))\n",
    "                                            \n",
    "                                    except Exception as e: \n",
    "                                        print(e)\n",
    "                                        \n",
    "                                        \n",
    "def train_models(X,\n",
    "                 tag,\n",
    "                 entity,\n",
    "                 dimension_params,\n",
    "                 clustering_params,\n",
    "                 vectorizer_params,\n",
    "                 ctfidf_params,\n",
    "                 count_mode=False\n",
    "               ):\n",
    "    count = 0\n",
    "    for _dim_model, _dim_model_params in dimension_params.items():\n",
    "        for _d_p in expand_dict_values(_dim_model_params):\n",
    "            for _cluster_model, _cluster_model_params in clustering_params.items():\n",
    "                for _c_p in expand_dict_values(_cluster_model_params):\n",
    "                    for _vec_model, _vec_params in vectorizer_params.items():\n",
    "                        for _v_p in expand_dict_values(_vec_params):                    \n",
    "                            for _idf_model, _idf_params in ctfidf_params.items():\n",
    "                                for _i_p in expand_dict_values(_idf_params):\n",
    "                                    if count_mode:\n",
    "                                        count+=1\n",
    "                                        continue \n",
    "                                    print('Creating file... ')\n",
    "                                    file_path = f\"models/{unidecode.unidecode(tag.lower().replace(' ','_').replace('-','').replace('|',''))}__\"+\\\n",
    "                                              f\"{entity}__\"+\\\n",
    "                                              f\"m__d_{_dim_model}_{'_'.join([str(i) for i in list(_d_p.values())])}\"+\\\n",
    "                                              f\"__c_{_cluster_model}_{'_'.join([str(i) for i in list(_c_p.values())])}\"+\\\n",
    "                                              f\"__v_{_vec_model}_{'_'.join([str(i) for i in list(_v_p.values())])}\"+\\\n",
    "                                              f\"__i_{_idf_model}_{'_'.join([str(i) for i in list(_i_p.values())])}.pkl\"\n",
    "                                    print(file_path)\n",
    "                                    \n",
    "                                    out_file_path = f\"models/{unidecode.unidecode(tag.lower().replace(' ','_').replace('-','').replace('|',''))}__\"+\\\n",
    "                                              f\"{entity}__\"+\\\n",
    "                                              f\"m__d_{_dim_model}_{'_'.join([str(i) for i in list(_d_p.values())])}\"+\\\n",
    "                                              f\"__c_{_cluster_model}_{'_'.join([str(i) for i in list(_c_p.values())])}\"+\\\n",
    "                                              f\"__v_{_vec_model}_{'_'.join([str(i) for i in list(_v_p.values())])}\"+\\\n",
    "                                              f\"__i_{_idf_model}_{'_'.join([str(i) for i in list(_i_p.values())])}_out.pkl\"\n",
    "                                    \n",
    "                                    outlier = False\n",
    "                                    \n",
    "  \n",
    "                                    # DIMENSION\n",
    "                                    if _dim_model == 'umap':\n",
    "                                        dim_model = UMAP(**_d_p)\n",
    "                                    if _dim_model == 'svd':\n",
    "                                        dim_model = TruncatedSVD(**_d_p)\n",
    "                                    if _dim_model == 'pca':\n",
    "                                        dim_model = PCA(**_d_p)\n",
    "                                        \n",
    "                                    # CLUSTERING\n",
    "                                    if _cluster_model == 'hdbscan':\n",
    "                                        cluster_model = HDBSCAN(**_c_p)\n",
    "                                        outlier = True\n",
    "                                    if _cluster_model == 'kmeans':\n",
    "                                        cluster_model = KMeans(**_c_p)\n",
    "                                    \n",
    "                                    # VECTORIZER\n",
    "                                    if _vec_model == 'CountVectorizer':\n",
    "                                        vec_model = CountVectorizer(**_v_p)\n",
    "                                    \n",
    "                                    if _idf_model == 'ctfidf':\n",
    "                                        idf_model = ClassTfidfTransformer(**_i_p)\n",
    "                                        \n",
    "                                    keybert = KeyBERTInspired()\n",
    "                                    mmr = MaximalMarginalRelevance(diversity=0.3)\n",
    "                                    \n",
    "                                    representation_model = {\n",
    "                                        \"KeyBERT\": keybert,\n",
    "                                        \"MMR\": mmr,\n",
    "                                    }\n",
    "                                    \n",
    "                                    if (os.path.exists(file_path) and os.path.exists(out_file_path)) or (os.path.exists(file_path) and ~outlier):\n",
    "                                        print('If the data changed please delete the pickle files rm models/*.pkl')\n",
    "                                        print('File already exists, running next parameter...')\n",
    "                                        continue\n",
    "                                    \n",
    "                                    sentence_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "                                    \n",
    "                                    topic_model = BERTopic(\n",
    "                                        embedding_model=sentence_model,\n",
    "                                        umap_model=dim_model,\n",
    "                                        hdbscan_model=cluster_model,\n",
    "                                        vectorizer_model=vec_model,\n",
    "                                        ctfidf_model=idf_model,\n",
    "                                        representation_model=representation_model,\n",
    "                                        top_n_words=10,\n",
    "                                        nr_topics=50,\n",
    "                                        verbose=True\n",
    "                                    )\n",
    "                                    \n",
    "                                    if ~os.path.exists(file_path):\n",
    "                                        print('training model...')\n",
    "                                        try:\n",
    "                                            _,__ = topic_model.fit_transform(X['ds_message'])\n",
    "\n",
    "                                            with open(file_path, 'wb') as file:    \n",
    "                                                pickle.dump(topic_model, file)\n",
    "                                            print('created file: ')\n",
    "                                            print(file_path)\n",
    "                                        except Exception as e:\n",
    "                                            print('ERROR: ')\n",
    "                                            print(e)\n",
    "                                    \n",
    "                                    if outlier:\n",
    "                                        try:\n",
    "                                            with open(file_path, 'rb') as file:\n",
    "                                                topic_model = pickle.load(file)\n",
    "\n",
    "                                            print('outlier')\n",
    "                                            \n",
    "                                            pred, prob = topic_model.transform(list(X.ds_message))\n",
    "                                            topics_df = topic_model.get_topic_info()\n",
    "                                            pred_df = pd.DataFrame(pred, columns = ['Topic'])\n",
    "                                            main_df = pd.merge(pred_df, topics_df, on=['Topic'], how='left')\n",
    "                                            main_df.index = X.index\n",
    "                                            main_df['Topic'] = main_df['Topic'].astype(str)\n",
    "                                            merged_df = pd.concat([X,main_df],axis=1)\n",
    "                                            \n",
    "\n",
    "                                            pred_out, prob_out = topic_model.fit_transform(list(merged_df[merged_df['Topic']=='-1'].ds_message))\n",
    "                                            with open(out_file_path, 'wb') as file:    \n",
    "                                                pickle.dump(topic_model, file)\n",
    "                                            print('created outlier file: ')\n",
    "                                            print(out_file_path)\n",
    "\n",
    "                                        except Exception as e:\n",
    "                                            print('ERRO OUTLIER')\n",
    "                                            print(e)\n",
    "                                        \n",
    "\n",
    "    if count_mode:\n",
    "        print('# diferent combinations will be tested:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130da1f-97f9-4c53-83da-9679503aecc5",
   "metadata": {},
   "source": [
    "## Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b762590-50d6-49a9-a441-8d542136d6b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tags_list = [\n",
    "    'w3 - credito pessoal | como adquirir',\n",
    "                    # 'b3 - duvida emprestimo',\n",
    "                           # 'n3 - emprestimo',\n",
    " # 'w3 - credito pessoal | negociacao via app',\n",
    "                           # 'q3 - empréstimo',\n",
    "                 # 'w3 - fgts | como adquirir',\n",
    "        # 'w3 - credito pessoal | amortização',\n",
    " #                                    'wb-emp',\n",
    " #                                   'wb-fgts',\n",
    " #                     'w3 - aviso empréstimo'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca90701-9bef-4fda-b8f6-00c69a1f7f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6cfe16b-fd70-4845-b174-9b805bef79a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5405eaa-5ab6-4b43-b3f8-1990e34ff383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####\n",
    "#### TRASFOMER\n",
    "####\n",
    "\n",
    "# sentence_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "model_path = 'transformers/all-MiniLM-L12-v2'\n",
    "# sentence_model.save(model_path)\n",
    "sentence_model = SentenceTransformer(model_path)\n",
    "\n",
    "####\n",
    "#### DIMENSIONS\n",
    "####\n",
    "umap_model_params = {\n",
    "    'n_neighbors':[\n",
    "        15,\n",
    "        # 30,\n",
    "                  ],\n",
    "    'n_components':[\n",
    "        # 5,\n",
    "        15,\n",
    "        # 30,\n",
    "                   ],\n",
    "    'min_dist':[0],\n",
    "    'metric':['cosine'],\n",
    "}\n",
    "svd_model_params = {\n",
    "    'n_components':[\n",
    "        5,\n",
    "        10,\n",
    "        30,\n",
    "    ],\n",
    "}\n",
    "pca_model_params = {\n",
    "    'n_components':[\n",
    "        5,\n",
    "        10,\n",
    "        30,\n",
    "    ],\n",
    "}\n",
    "\n",
    "dimension_params = {\n",
    "    'umap':umap_model_params,\n",
    "    # 'svd':svd_model_params,\n",
    "    # 'pca':pca_model_params,\n",
    "}\n",
    "\n",
    "####\n",
    "#### CLUSTERS\n",
    "####\n",
    "hdbscan_model_params = {\n",
    "    'min_cluster_size':[\n",
    "        10,\n",
    "        # 30,\n",
    "        35,\n",
    "        # 50,\n",
    "    ],\n",
    "    'metric':[\n",
    "        'euclidean',\n",
    "    ],\n",
    "    'cluster_selection_method':['eom'],\n",
    "    'prediction_data':[True],\n",
    "}\n",
    "\n",
    "kmeans_model_params = {\n",
    "    'n_clusters':[\n",
    "        # 10,\n",
    "        # 30,\n",
    "        50,\n",
    "    ]\n",
    "}\n",
    "clustering_params = {\n",
    "    'hdbscan':hdbscan_model_params,\n",
    "    # 'kmeans':kmeans_model_params,\n",
    "}\n",
    "\n",
    "####\n",
    "#### VECTORIZERS\n",
    "####\n",
    "vectorizer_model_params = {\n",
    "    'ngram_range': [\n",
    "        (1,1),\n",
    "        (1,2),\n",
    "        (1,3),\n",
    "        # (2,4),\n",
    "        # (1,4)\n",
    "    ],\n",
    "}\n",
    "\n",
    "vectorizer_params = {\n",
    "    'CountVectorizer': vectorizer_model_params\n",
    "}\n",
    "\n",
    "####\n",
    "#### CTFIDF\n",
    "####\n",
    "ctfidf_model_params = {\n",
    "    'reduce_frequent_words': [\n",
    "        # False,\n",
    "        True,\n",
    "    ],\n",
    "}\n",
    "\n",
    "ctfidf_params = {\n",
    "    'ctfidf': ctfidf_model_params\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a136123-e35a-49ee-a020-0dcc191ed5d5",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69acbe8-1bbd-43d1-8c79-59cacdfa7be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remove_sentences = {\n",
    "    'cliente' : [\n",
    "        'outros assuntos',\n",
    "        'falar outra coisa',\n",
    "        'voltar pro comeco',\n",
    "        'ok',\n",
    "        'outros assuntos falar outra coisa',\n",
    "        'opcoes',\n",
    "        'nao',\n",
    "        'outro assunto',\n",
    "        '',\n",
    "        'falar outro assunto',\n",
    "        'ja resolvi problema',\n",
    "        'falar atendente',\n",
    "        'falar outra coisa emprestimo',\n",
    "        'certo',\n",
    "        'oi',\n",
    "        'nao obrigado',\n",
    "        'responder mensagem antes',\n",
    "        'falar outra coisa emprestimo pessoal',\n",
    "        'opcoes falar outra coisa',\n",
    "        'ok obrigado',\n",
    "        'ok obrigada',\n",
    "        'obrigada',\n",
    "        'outra coisa sobre fatura',\n",
    "        'obrigado',\n",
    "        'nao obrigada',\n",
    "        'ta',\n",
    "        'so',\n",
    "        'ok oi',\n",
    "        'sim',\n",
    "        'voltar',\n",
    "        'outras dicas',\n",
    "        'outra coisa',\n",
    "        'ta bom',\n",
    "        'obg',\n",
    "        'voltar pro comeco outros assuntos',\n",
    "        'blz',\n",
    "        'ola',\n",
    "        'bom dia',\n",
    "        'boa tarde',\n",
    "        'boa noite',\n",
    "        'outros assuntos voltar',\n",
    "        'voltar pras opcoes antes',\n",
    "        'outros assuntos emprestimo fgts',\n",
    "        'emprestimo pessoal',\n",
    "        'emprestimo fgts',\n",
    "        'emprestimo',\n",
    "        'limite credito',\n",
    "        'fatura cartao',\n",
    "        'aumento limite',\n",
    "        'pix',\n",
    "        'emprestimo pessoal',\n",
    "        'tudo bem',\n",
    "        'entendi',\n",
    "        'aguardo',\n",
    "        'esponder mensagem antes emprestimo',\n",
    "        'nenhuma opcoes',\n",
    "        'nenhuma dessas',\n",
    "        'nao ainda duvidas',\n",
    "        'ainda preciso ajuda',\n",
    "        'nenhuma opcoes nenhuma dessas',\n",
    "        'nao ainda duvidas nenhuma opcoes',\n",
    "        'sim ainda preciso ajuda',\n",
    "        'ok ok'\n",
    "        \n",
    "    ],\n",
    "    'agente': [\n",
    "        ''\n",
    "    ]\n",
    "}    \n",
    "\n",
    "\n",
    "remove_pattern = {\n",
    "    'cliente' : [\n",
    "        'obrig'\n",
    "    ],\n",
    "    'agente': [\n",
    "        ''\n",
    "    ]\n",
    "}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703b725-c5c7-4372-8332-679a1614e81c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = merge_consecutive_chats(raw_data, tag)\n",
    "X = bert_pipe.fit_transform(df[(df['ds_entidade']=='cliente')][['ds_message']]) \n",
    "X = X[~X.ds_message.isin(remove_sentences['cliente'])]\n",
    "X = X[~X.ds_message.str.contains(fr\"{'|'.join(remove_pattern['cliente'])}\", regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585edfe9-c7ab-4660-a92c-1f089a31b5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tag in tags_list:\n",
    "    print(tag)\n",
    "    df = merge_consecutive_chats(raw_data, tag)\n",
    "    display(df.groupby('ds_entidade').count())\n",
    "    \n",
    "    if tag[0]=='w' or tag[0]=='n':\n",
    "        entity = 'cliente'\n",
    "        print(entity)\n",
    "        X = bert_pipe.fit_transform(df[(df['ds_entidade']==entity)][['ds_message']]) \n",
    "        X = X[~X.ds_message.isin(remove_sentences[entity])]\n",
    "        X = X[~X.ds_message.str.contains(fr\"{'|'.join(remove_pattern[entity])}\", regex=True)]\n",
    "        print('Messages after sentence removal: ',X.shape)\n",
    "\n",
    "        train_models(X,\n",
    "                 tag,\n",
    "                 entity,\n",
    "                 dimension_params,\n",
    "                 clustering_params,\n",
    "                 vectorizer_params,\n",
    "                 ctfidf_params,\n",
    "                 count_mode=False)\n",
    "        \n",
    "        entity = 'agente'\n",
    "        print(entity)\n",
    "        X = bert_pipe.fit_transform(\n",
    "            df[(df['ds_entidade']==entity)][['ds_message']]) \n",
    "        \n",
    "        train_models(X,\n",
    "                 tag,\n",
    "                 entity,\n",
    "                 dimension_params,\n",
    "                 clustering_params,\n",
    "                 vectorizer_params,\n",
    "                 ctfidf_params,\n",
    "                 count_mode=False)\n",
    "    else:\n",
    "        entity = 'cliente'\n",
    "        print(entity)\n",
    "        X = bert_pipe.fit_transform(df[(df['ds_entidade']==entity)][['ds_message']]) \n",
    "        # X = X[~X.ds_message.isin(remove_sentences[entity])]\n",
    "        # X = X[~X.ds_message.str.contains(fr\"{'|'.join(remove_pattern[entity])}\", regex=True)]\n",
    "        print('Messages after sentence removal: ',X.shape)\n",
    "\n",
    "        train_models(X,\n",
    "                 tag,\n",
    "                 entity,\n",
    "                 dimension_params,\n",
    "                 clustering_params,\n",
    "                 vectorizer_params,\n",
    "                 ctfidf_params,\n",
    "                 count_mode=False)\n",
    "        \n",
    "        entity = 'bot'\n",
    "        print(entity)\n",
    "        X = bert_pipe.fit_transform(df[(df['ds_entidade']==entity)][['ds_message']]) \n",
    "        \n",
    "        train_models(X,\n",
    "                 tag,\n",
    "                 entity,\n",
    "                 dimension_params,\n",
    "                 clustering_params,\n",
    "                 vectorizer_params,\n",
    "                 ctfidf_params,\n",
    "                 count_mode=False)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88972bd4-96a9-4333-a945-0c4a734ff241",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373df32b-2072-4290-a269-0f236c8ff057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tag in tags_list:\n",
    "    print(tag)\n",
    "    if tag[0]=='w' or tag[0]=='n':\n",
    "        entities = [\n",
    "            'cliente',\n",
    "            'agente'\n",
    "        ]\n",
    "    else: \n",
    "        entities = [\n",
    "            'cliente',\n",
    "            'bot'\n",
    "        ]\n",
    "\n",
    "    for entity in entities:\n",
    "        load_models(tag,\n",
    "                    entity,\n",
    "                    dimension_params,\n",
    "                    clustering_params,\n",
    "                    vectorizer_params,\n",
    "                    ctfidf_params)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105fe36-2f85-4a48-8d46-8928fce8a433",
   "metadata": {},
   "source": [
    "## Best params per tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b161bc-00b4-451a-81ba-e5833ff26a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params_per_tag = {\n",
    "'w3 - credito pessoal | como adquirir': {\n",
    "    'cliente' : 'models/w3__credito_pessoal__como_adquirir__cliente__m__d_umap_15_15_0_cosine__c_hdbscan_35_euclidean_eom_True__v_CountVectorizer_(1, 2)__i_ctfidf_True.pkl',\n",
    "    'agente':'models/w3__credito_pessoal__como_adquirir__agente__m__d_umap_15_15_0_cosine__c_hdbscan_35_euclidean_eom_True__v_CountVectorizer_(1, 2)__i_ctfidf_True.pkl',\n",
    "},\n",
    "'b3 - duvida emprestimo': {\n",
    "    'cliente': 'models/b3__duvida_emprestimo__cliente__m__d_umap_15_15_0_cosine__c_hdbscan_35_euclidean_eom_True__v_CountVectorizer_(1, 2)__i_ctfidf_True.pkl',\n",
    "    'bot':'models/b3__duvida_emprestimo__bot__m__d_umap_15_15_0_cosine__c_hdbscan_35_euclidean_eom_True__v_CountVectorizer_(1, 2)__i_ctfidf_True.pkl',\n",
    "},\n",
    "'n3 - emprestimo':{\n",
    "    'cliente':'models/n3__emprestimo__cliente__m__d_umap_15_15_0_cosine__c_hdbscan_10_euclidean_eom_True__v_CountVectorizer_(1, 1)__i_ctfidf_True.pkl',\n",
    "    'agente':'models/n3__emprestimo__agente__m__d_umap_15_15_0_cosine__c_hdbscan_10_euclidean_eom_True__v_CountVectorizer_(1, 1)__i_ctfidf_True.pkl',\n",
    "},\n",
    "'w3 - credito pessoal | negociacao via app' :{\n",
    "    'cliente': 'models/w3__credito_pessoal__negociacao_via_app__cliente__m__d_umap_15_15_0_cosine__c_hdbscan_10_euclidean_eom_True__v_CountVectorizer_(1, 1)__i_ctfidf_True.pkl',\n",
    "    'agente': 'models/w3__credito_pessoal__negociacao_via_app__agente__m__d_umap_15_15_0_cosine__c_hdbscan_10_euclidean_eom_True__v_CountVectorizer_(1, 3)__i_ctfidf_True.pkl',\n",
    "},\n",
    "'q3 - empréstimo' : {\n",
    "    'cliente':'models/q3__emprestimo__cliente__m__d_umap_15_15_0_cosine__c_hdbscan_10_euclidean_eom_True__v_CountVectorizer_(1, 3)__i_ctfidf_True.pkl',\n",
    "    'bot':'models/q3__emprestimo__bot__m__d_umap_15_15_0_cosine__c_hdbscan_10_euclidean_eom_True__v_CountVectorizer_(1, 3)__i_ctfidf_True.pkl',\n",
    "},\n",
    "'w3 - fgts | como adquirir' : {\n",
    "    'cliente':'models/w3__fgts__como_adquirir__cliente__m__d_umap_15_15_0_cosine__c_hdbscan_35_euclidean_eom_True__v_CountVectorizer_(1, 2)__i_ctfidf_True.pkl',\n",
    "    'agente':'models/w3__fgts__como_adquirir__agente__m__d_umap_15_15_0_cosine__c_hdbscan_10_euclidean_eom_True__v_CountVectorizer_(1, 1)__i_ctfidf_True.pkl',\n",
    "},\n",
    "'w3 - credito pessoal | amortização': {\n",
    "    'cliente':'models/w3__credito_pessoal__amortizacao__cliente__m__d_umap_15_15_0_cosine__c_hdbscan_10_euclidean_eom_True__v_CountVectorizer_(1, 3)__i_ctfidf_True.pkl',\n",
    "    'agente':'models/w3__credito_pessoal__amortizacao__agente__m__d_umap_15_15_0_cosine__c_hdbscan_10_euclidean_eom_True__v_CountVectorizer_(1, 3)__i_ctfidf_True.pkl',\n",
    "},\n",
    "# 'wb-emp',\n",
    "# 'wb-fgts',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86322b33-7d43-4faf-98d0-135259898700",
   "metadata": {},
   "source": [
    "## Write dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444c352-f446-4c93-b316-12f71c983c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tag, values in best_params_per_tag.items():\n",
    "    print(tag)\n",
    "    error = False\n",
    "    for ent, file_path in values.items():\n",
    "        print('entidade:', ent)\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                topic_model = pickle.load(file)\n",
    "\n",
    "            df = merge_consecutive_chats(raw_data, tag)\n",
    "            if (tag[0]=='w' or tag[0]=='n') and ent=='cliente':\n",
    "                X = bert_pipe.fit_transform(df[(df['ds_entidade']=='cliente')][['ds_message']])\n",
    "                X = X[~X.ds_message.isin(remove_sentences[ent])]\n",
    "                X = X[~X.ds_message.str.contains(fr\"{'|'.join(remove_pattern[ent])}\", regex=True)]\n",
    "                print('X dim:',X.shape)\n",
    "            else:\n",
    "                X = bert_pipe.fit_transform(df[(df['ds_entidade']==ent)][['ds_message']])\n",
    "                print('X dim:',X.shape)\n",
    "            \n",
    "            pred, prob = topic_model.transform(list(X.ds_message))\n",
    "            topics_df = topic_model.get_topic_info()\n",
    "            pred_df = pd.DataFrame(pred, columns = ['Topic'])\n",
    "            pred_df['prob'] = prob \n",
    "            \n",
    "            main_df = pd.merge(pred_df, topics_df, on=['Topic'], how='left')\n",
    "            main_df.index = X.index\n",
    "            main_df['Topic'] = main_df['Topic'].astype(str)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('ERRO')\n",
    "            print(e)\n",
    "            error = True\n",
    "        \n",
    "        ## write dataframe\n",
    "        merged_df = pd.concat([df,main_df],axis=1)\n",
    "        display(merged_df)\n",
    "        \n",
    "        try:\n",
    "            if 'hdbscan' in file_path:\n",
    "                    print('outlier')\n",
    "                    if (tag[0]=='w' or tag[0]=='n') and ent=='cliente':\n",
    "                        X = bert_pipe.fit_transform(merged_df[merged_df['Topic']=='-1'][['ds_message']])\n",
    "                        X = X[~X.ds_message.isin(remove_sentences[ent])]\n",
    "                        X = X[~X.ds_message.str.contains(fr\"{'|'.join(remove_pattern[ent])}\", regex=True)]\n",
    "                        print('OUT X dim:',X.shape)\n",
    "                    else:\n",
    "                        X = bert_pipe.fit_transform(merged_df[merged_df['Topic']=='-1'][['ds_message']])\n",
    "                        print('OUT X dim:',X.shape)\n",
    "                    \n",
    "                    with open(file_path.split('.')[0]+ '_out' + '.pkl', 'rb') as file:\n",
    "                        topic_model = pickle.load(file)\n",
    "                    \n",
    "                    pred_out, prob_out = topic_model.transform(list(X.ds_message))\n",
    "                    topics_df = topic_model.get_topic_info()\n",
    "                    pred_out_df = pd.DataFrame(pred_out, columns = ['Topic'])\n",
    "                    pred_out_df['prob'] = prob\n",
    "                    out_df = pd.merge(pred_out_df, topics_df, on=['Topic'], how='left')\n",
    "                    out_df.index = merged_df[merged_df['Topic']=='-1'].index\n",
    "                    out_df['Topic'] = '-1_'+ out_df['Topic'].astype(str)\n",
    "                    out_df['Name'] = '-1_'+ out_df['Name'].astype(str)\n",
    "                    # display(out_df)\n",
    "                    \n",
    "                    # cliente[model_cols].fillna(agente[model_cols]) \n",
    "                    cols = ['Topic','Count','Name','Representation','KeyBERT','MMR','Representative_Docs']\n",
    "                    merged_df.loc[merged_df['Topic']=='-1', cols] = out_df\n",
    "        except Exception as e:\n",
    "            print('ERRO OUTLIER')\n",
    "            print(e)\n",
    "            error = True\n",
    "            \n",
    "\n",
    "            \n",
    "        if not error:\n",
    "            merged_df.to_parquet(f'data/{tag}_{ent}_merged.parquet')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c4928-7826-46df-99eb-c536a7d4c5fd",
   "metadata": {},
   "source": [
    "## Send to dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d874afc-6a67-4a58-8149-4e01642bf952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# Define staging_path\n",
    "staging_path = \n",
    "\n",
    "\n",
    "# Define boto3 session\n",
    "boto3_session = boto3_session = boto3.Session(region_name=\"sa-east-1\")\n",
    "\n",
    "# Clean temporary folder\n",
    "wr.s3.delete_objects(staging_path, boto3_session=boto3_session)\n",
    "\n",
    "# Run query\n",
    "df_ep = wr.athena.read_sql_query(\n",
    "    sql=query,\n",
    "    database=None,\n",
    "    ctas_approach=False,\n",
    "    s3_output=staging_path,\n",
    ").drop_duplicates()\\\n",
    " .add_suffix('_ep')\\\n",
    " .rename(columns={'cd_perfil_ep_ep':'cd_perfil_ep',\n",
    "                  'ds_perfil_ep_ep':'ds_perfil_ep'})\n",
    "wr.s3.delete_objects(staging_path, boto3_session=boto3_session)\n",
    "                                 \n",
    "\n",
    "query = f\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Define staging_path\n",
    "staging_path = \n",
    "\n",
    "# Define boto3 session\n",
    "boto3_session = boto3_session = boto3.Session(region_name=\"sa-east-1\")\n",
    "\n",
    "# Clean temporary folder\n",
    "wr.s3.delete_objects(staging_path, boto3_session=boto3_session)\n",
    "\n",
    "# Run query\n",
    "df_fgts = wr.athena.read_sql_query(\n",
    "    sql=query,\n",
    "    database=None,\n",
    "    ctas_approach=False,\n",
    "    s3_output=staging_path,\n",
    ").drop_duplicates()\\\n",
    " .add_suffix('_fgts')\\\n",
    " .rename(columns={'cd_perfil_ep_ep':'cd_perfil_fgts',\n",
    "              'ds_perfil_ep_ep':'ds_perfil_fgts'})\n",
    "wr.s3.delete_objects(staging_path, boto3_session=boto3_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e694a9-72b4-46db-994a-5f776b69576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.set_index('dt_criacao_chat').groupby(pd.Grouper(freq='W')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b268d-1022-4e6e-b314-43695cb8d102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dash_df = []\n",
    "for tag, values in best_params_per_tag.items():\n",
    "    print(tag, values)\n",
    "    try:\n",
    "        cliente = pd.read_parquet(f'data/{tag}_{list(values.keys())[0]}_merged.parquet')\n",
    "        display(cliente.head(3))\n",
    "        agente = pd.read_parquet(f'data/{tag}_{list(values.keys())[1]}_merged.parquet')\n",
    "        display(cliente.head(3))\n",
    "    except:\n",
    "        print('could not read some of the files')\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        model_cols = ['ds_entidade',\n",
    "                      'group',\n",
    "                      'ds_tags',\n",
    "                      'ds_message',\n",
    "                      'dt_criacao_chat',\n",
    "                      'dt_envio_mensagem',\n",
    "                      'Topic',\n",
    "                      'Count',\n",
    "                      'Name',\n",
    "                      'Representation',\n",
    "                      'KeyBERT',\n",
    "                      'MMR',\n",
    "                      'Representative_Docs']\n",
    "\n",
    "        cliente[model_cols] = cliente[model_cols].fillna(agente[model_cols]) \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        cliente['Probability'] = 1\n",
    "        agente['Probability'] = 1\n",
    "        model_cols = ['ds_entidade',\n",
    "                      'group',\n",
    "                      'ds_tags',\n",
    "                      'ds_message',\n",
    "                      'dt_criacao_chat',\n",
    "                      'dt_envio_mensagem',\n",
    "                      'Topic',\n",
    "                      'Count',\n",
    "                      'Name',\n",
    "                      'Representation',\n",
    "                      'KeyBERT',\n",
    "                      'MMR',\n",
    "                      'Representative_Docs']\n",
    "\n",
    "        cliente[model_cols] = cliente[model_cols].fillna(agente[model_cols]) \n",
    "        \n",
    "\n",
    "    qna = cliente.copy()\n",
    "    qna['Document_before'] = qna.groupby(['id_customer','id_issue']).ds_message.shift(1).astype(str)\n",
    "    qna['Topic_before'] = qna.groupby(['id_customer','id_issue']).Topic.shift(1).astype(str)\n",
    "\n",
    "    qna['Document_after'] = qna.groupby(['id_customer','id_issue']).ds_message.shift(-1).astype(str)\n",
    "    qna['Topic_after'] = qna.groupby(['id_customer','id_issue']).Topic.shift(-1).astype(str)\n",
    "\n",
    "    qna['chatmonth'] = qna.dt_criacao_chat.dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "    qna = pd.merge(qna,\n",
    "             df_ep,\n",
    "             how='left',\n",
    "             left_on=['id_customer','chatmonth'],\n",
    "             right_on=['customer_id_ep','mes_elegivel_modelo_ep'])\n",
    "\n",
    "    qna = pd.merge(qna,\n",
    "             df_fgts,\n",
    "             how='left',\n",
    "             left_on=['id_customer','chatmonth'],\n",
    "             right_on=['customer_id_fgts','mes_elegivel_modelo_fgts'])\n",
    "\n",
    "\n",
    "    qna_dash = qna[qna.Name.notna()].copy()\n",
    "\n",
    "    qna_dash['fmt_name'] = qna_dash.Name.apply(lambda x: '2_'+'_'.join(x.replace('-1_','').split('_')[1:])\n",
    "                                                        if '-1' in x\n",
    "                                                        else '_'.join(x.replace('-1_','').split('_')[1:])\n",
    "                              )\n",
    "    qna_dash['fmt_representation']=qna_dash.KeyBERT.apply(lambda x: ' '.join(x))\n",
    "    qna_dash['fmt_representative_docs']=qna_dash.Representative_Docs.apply(lambda x: ' '.join(x))\n",
    "    qna_dash['dt_envio_mensagem']=pd.to_datetime(qna_dash.dt_envio_mensagem, format='%Y-%m-%d')\n",
    "    qna_dash['fmt_dt_envio_mensagem']=pd.to_datetime(qna_dash.dt_envio_mensagem, format='%Y-%m-%d').dt.floor('D')\n",
    "    qna_dash['tag'] = tag\n",
    "    \n",
    "    print(qna_dash.shape)    \n",
    "    dash_df.append(qna_dash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e5b76-1ed8-4546-b4fb-a299b1a856b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dashboard = pd.concat(dash_df,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a7da3-4e07-4a46-9435-3ef47f3d5d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define staging_path\n",
    "staging_path = \n",
    "\n",
    "# Define boto3 session\n",
    "boto3_session = boto3_session = boto3.Session(region_name=\"sa-east-1\")\n",
    "\n",
    "# Clean temporary folder\n",
    "wr.s3.delete_objects(staging_path, boto3_session=boto3_session)\n",
    "\n",
    "query =\n",
    "# Run query\n",
    "tmp = wr.athena.read_sql_query(\n",
    "    sql=query,\n",
    "    database=None,\n",
    "    ctas_approach=False,\n",
    "    s3_output=staging_path,\n",
    ")\n",
    "display(tmp)\n",
    "wr.s3.delete_objects(staging_path, boto3_session=boto3_session)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:sa-east-1:782484402741:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
